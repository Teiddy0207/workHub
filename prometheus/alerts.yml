groups:
  - name: system_alerts
    interval: 30s
    rules:
      # Alert: High CPU Usage
      - alert: HighCPUUsage
        expr: |
          100 - (avg by (instance) (rate(process_cpu_seconds_total{job="prometheus"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
          service: prometheus
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage is above 80% for more than 5 minutes on {{ $labels.instance }}"

      # Alert: High Memory Usage
      - alert: HighMemoryUsage
        expr: |
          (process_resident_memory_bytes{job="prometheus"} / 1024 / 1024 / 1024) > 2
        for: 5m
        labels:
          severity: warning
          service: prometheus
        annotations:
          summary: "High memory usage detected"
          description: "Memory usage is above 2GB for more than 5 minutes on {{ $labels.instance }}"

  - name: postgresql_alerts
    interval: 30s
    rules:
      # Alert: PostgreSQL Down
      - alert: PostgreSQLDown
        expr: up{job="postgres"} == 0
        for: 1m
        labels:
          severity: critical
          service: postgresql
        annotations:
          summary: "PostgreSQL exporter is down"
          description: "PostgreSQL exporter has been down for more than 1 minute"

      # Alert: High Database Connections
      - alert: HighPostgreSQLConnections
        expr: |
          pg_stat_database_numbackends{datname="workhub_db"} > 80
        for: 5m
        labels:
          severity: warning
          service: postgresql
        annotations:
          summary: "High number of PostgreSQL connections"
          description: "PostgreSQL has {{ $value }} active connections (threshold: 80) on database {{ $labels.datname }}"

      # Alert: Slow Database Queries
      - alert: SlowPostgreSQLQueries
        expr: |
          avg(pg_stat_activity_max_tx_duration{state="active"}) > 5
        for: 5m
        labels:
          severity: warning
          service: postgresql
        annotations:
          summary: "Slow PostgreSQL queries detected"
          description: "PostgreSQL has queries running longer than 5 seconds on {{ $labels.instance }}"

      # Alert: Low Cache Hit Ratio
      - alert: LowPostgreSQLCacheHitRatio
        expr: |
          (
            sum(rate(pg_stat_database_blks_hit[5m])) 
            / 
            (sum(rate(pg_stat_database_blks_hit[5m])) + sum(rate(pg_stat_database_blks_read[5m])))
          ) * 100 < 90
        for: 10m
        labels:
          severity: warning
          service: postgresql
        annotations:
          summary: "Low PostgreSQL cache hit ratio"
          description: "PostgreSQL cache hit ratio is below 90% (current: {{ $value }}%)"

      # Alert: High Database Size
      - alert: HighPostgreSQLDatabaseSize
        expr: |
          (pg_database_size_bytes{datname="workhub_db"} / 1024 / 1024 / 1024) > 50
        for: 5m
        labels:
          severity: warning
          service: postgresql
        annotations:
          summary: "PostgreSQL database size is high"
          description: "Database {{ $labels.datname }} size is {{ $value }}GB (threshold: 50GB)"

  - name: redis_alerts
    interval: 30s
    rules:
      # Alert: Redis Down
      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
          service: redis
        annotations:
          summary: "Redis is down"
          description: "Redis exporter has been down for more than 1 minute"

      # Alert: High Redis Memory Usage
      - alert: HighRedisMemoryUsage
        expr: |
          (redis_memory_used_bytes / redis_memory_max_bytes) * 100 > 90
        for: 5m
        labels:
          severity: warning
          service: redis
        annotations:
          summary: "High Redis memory usage"
          description: "Redis memory usage is above 90% (current: {{ $value }}%)"

      # Alert: High Redis Connections
      - alert: HighRedisConnections
        expr: |
          redis_connected_clients > 100
        for: 5m
        labels:
          severity: warning
          service: redis
        annotations:
          summary: "High number of Redis connections"
          description: "Redis has {{ $value }} connected clients (threshold: 100)"

      # Alert: Low Redis Hit Rate
      - alert: LowRedisHitRate
        expr: |
          (
            rate(redis_keyspace_hits_total[5m]) 
            / 
            (rate(redis_keyspace_hits_total[5m]) + rate(redis_keyspace_misses_total[5m]))
          ) * 100 < 80
        for: 10m
        labels:
          severity: warning
          service: redis
        annotations:
          summary: "Low Redis hit rate"
          description: "Redis hit rate is below 80% (current: {{ $value }}%)"

  - name: prometheus_alerts
    interval: 30s
    rules:
      # Alert: Prometheus Target Down
      - alert: PrometheusTargetDown
        expr: up == 0
        for: 2m
        labels:
          severity: critical
          service: monitoring
        annotations:
          summary: "Prometheus target is down"
          description: "Target {{ $labels.job }}/{{ $labels.instance }} has been down for more than 2 minutes"

      # Alert: Prometheus High Scrape Latency
      - alert: PrometheusHighScrapeLatency
        expr: |
          prometheus_target_interval_length_seconds{quantile="0.99"} > 10
        for: 5m
        labels:
          severity: warning
          service: monitoring
        annotations:
          summary: "High Prometheus scrape latency"
          description: "Prometheus scrape latency is above 10s on {{ $labels.instance }}"

  - name: application_alerts
    interval: 30s
    rules:
      # Alert: High Error Rate (cần có metric từ ứng dụng)
      # Ví dụ: nếu ứng dụng expose metric error_rate_total
      - alert: HighErrorRate
        expr: |
          rate(http_requests_total{status=~"5.."}[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
          service: application
        annotations:
          summary: "High error rate detected"
          description: "Error rate is above 0.1 req/s (current: {{ $value }} req/s)"

      # Alert: High Request Latency (cần có metric từ ứng dụng)
      - alert: HighRequestLatency
        expr: |
          histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m])) > 1
        for: 5m
        labels:
          severity: warning
          service: application
        annotations:
          summary: "High request latency"
          description: "99th percentile request latency is above 1s (current: {{ $value }}s)"

